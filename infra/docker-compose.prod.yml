version: "3.9"

# Production overrides - use with: docker compose -f docker-compose.yml -f docker-compose.prod.yml up

services:
  api:
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4
    restart: always
    deploy:
      resources:
        limits:
          memory: 512M

  worker-support:
    restart: always
    deploy:
      resources:
        limits:
          memory: 512M

  worker-leads:
    restart: always
    deploy:
      resources:
        limits:
          memory: 512M

  frontend:
    restart: always

  db:
    restart: always

  redis:
    restart: always

  # Optional: llama.cpp server for local 7B inference
  # Uncomment and configure if you have GPU resources
  # llama:
  #   image: ghcr.io/ggerganov/llama.cpp:server
  #   ports:
  #     - "8081:8081"
  #   volumes:
  #     - ./models:/models
  #   command: >
  #     --model /models/mistral-7b-instruct-v0.3.Q4_K_M.gguf
  #     --port 8081
  #     --host 0.0.0.0
  #     --ctx-size 4096
  #     --n-gpu-layers 35
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
